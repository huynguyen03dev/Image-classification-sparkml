{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2520dd1d-7aae-41cd-bba4-75979fb2ac44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "from botocore.client import Config\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "\n",
    "# Spark ML Imports\n",
    "from pyspark.ml.feature import StringIndexer, PCA\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "\n",
    "print(\"=== Starting MinIO Image Training Script (Pandas UDF + Spark ML) ===\")\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration and Data Loading\n",
    "# -----------------------------\n",
    "print(\"Configuring MinIO client...\")\n",
    "minio_endpoint = 'http://minio:9000'\n",
    "minio_access_key = 'minioadmin'\n",
    "minio_secret_key = 'minioadmin'\n",
    "bucket_name = 'dev'\n",
    "local_image_dir = '/tmp/minio_images/'\n",
    "\n",
    "print(\"Initializing MinIO client...\")\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    endpoint_url=minio_endpoint,\n",
    "    aws_access_key_id=minio_access_key,\n",
    "    aws_secret_access_key=minio_secret_key,\n",
    "    config=Config(signature_version='s3v4')\n",
    ")\n",
    "\n",
    "if not os.path.exists(local_image_dir):\n",
    "    print(\"Creating local directory for images:\", local_image_dir)\n",
    "    os.makedirs(local_image_dir)\n",
    "else:\n",
    "    print(\"Local image directory exists:\", local_image_dir)\n",
    "\n",
    "print(\"Listing and downloading images from MinIO bucket...\")\n",
    "image_extensions = {'.jpg', '.jpeg', '.png', '.gif', '.bmp'}\n",
    "downloaded_files = []  # each entry: (local_file_path, label)\n",
    "\n",
    "list_response = s3.list_objects_v2(Bucket=bucket_name)\n",
    "if 'Contents' in list_response:\n",
    "    for obj in list_response['Contents']:\n",
    "        key = obj['Key']  # e.g., \"antelope/27a5369441.jpg\"\n",
    "        if any(key.lower().endswith(ext) for ext in image_extensions):\n",
    "            local_file_path = os.path.join(local_image_dir, key)\n",
    "            os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n",
    "            s3.download_file(bucket_name, key, local_file_path)\n",
    "            label = key.split('/')[0] if \"/\" in key else \"unknown\"\n",
    "            downloaded_files.append((local_file_path, label))\n",
    "else:\n",
    "    print(\"No objects found in bucket:\", bucket_name)\n",
    "\n",
    "total_downloaded = len(downloaded_files)\n",
    "print(\"Finished downloading images. Total images downloaded:\", total_downloaded)\n",
    "\n",
    "print(\"Walking local image directory to list images...\")\n",
    "image_data_list = []\n",
    "for root, dirs, files in os.walk(local_image_dir):\n",
    "    for file in files:\n",
    "        if any(file.lower().endswith(ext) for ext in image_extensions):\n",
    "            file_path = os.path.join(root, file)\n",
    "            rel_path = os.path.relpath(file_path, local_image_dir)\n",
    "            label = rel_path.split(os.sep)[0]\n",
    "            image_data_list.append((file_path, label))\n",
    "print(\"Total image files found (by walking directory):\", len(image_data_list))\n",
    "\n",
    "# -----------------------------\n",
    "# Create Spark Session and DataFrame\n",
    "# -----------------------------\n",
    "print(\"Creating Spark session...\")\n",
    "spark = SparkSession.builder.appName(\"MinIO Image Training PandasUDF + Spark ML\").getOrCreate()\n",
    "\n",
    "print(\"Creating Spark DataFrame with image paths and labels...\")\n",
    "df = spark.createDataFrame(image_data_list, schema=[\"image_path\", \"label\"]).persist()\n",
    "df.show(5, truncate=False)\n",
    "\n",
    "# -----------------------------\n",
    "# Define Image Processing via Pandas UDF\n",
    "# -----------------------------\n",
    "print(\"Defining image loading function...\")\n",
    "\n",
    "def load_image(file_path):\n",
    "    try:\n",
    "        img = Image.open(file_path)\n",
    "        img = img.resize((128, 128))  # resize to fixed size\n",
    "        img = np.array(img)\n",
    "        # Debug: print out shape and dtype for each image load (optional, may slow processing)\n",
    "        print(\"Loaded image:\", file_path, \"-> shape:\", img.shape, \"dtype:\", img.dtype)\n",
    "        if img.ndim == 2:\n",
    "            img = np.stack((img,) * 3, axis=-1)\n",
    "        elif img.shape[-1] == 4:\n",
    "            img = img[:, :, :3]\n",
    "        if img.shape != (128, 128, 3):\n",
    "            raise ValueError(f\"Unexpected image shape: {img.shape}\")\n",
    "        return img.astype(\"float32\") / 255.0  # Normalize image\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image {file_path}: {e}\")\n",
    "        return np.zeros((128, 128, 3), dtype=np.float32)\n",
    "\n",
    "print(\"Defining Pandas UDF to preprocess images...\")\n",
    "\n",
    "@pandas_udf(ArrayType(FloatType()))\n",
    "def process_image_udf(path_series: pd.Series) -> pd.Series:\n",
    "    # Apply the load_image function to each file path and flatten the image to a list of floats\n",
    "    return path_series.apply(lambda path: load_image(path).flatten().tolist())\n",
    "\n",
    "print(\"Applying Pandas UDF to DataFrame (this may take a while)...\")\n",
    "df = df.withColumn(\"image_data\", process_image_udf(\"image_path\"))\n",
    "# Show a small subset without printing huge arrays\n",
    "df.select(\"image_path\", \"label\").show(5, truncate=50)\n",
    "\n",
    "# -----------------------------\n",
    "# Convert Feature Array to Dense Vector\n",
    "# -----------------------------\n",
    "print(\"Converting image_data to DenseVector...\")\n",
    "\n",
    "def to_vector(arr):\n",
    "    # Convert list of floats to a DenseVector\n",
    "    return Vectors.dense(arr) if arr is not None else None\n",
    "\n",
    "from pyspark.sql.functions import udf as spark_udf\n",
    "vector_udf = spark_udf(to_vector, VectorUDT())\n",
    "df = df.withColumn(\"features\", vector_udf(\"image_data\"))\n",
    "df.select(\"image_path\", \"label\", \"features\").show(5, truncate=50)\n",
    "\n",
    "# -----------------------------\n",
    "# Label Indexing\n",
    "# -----------------------------\n",
    "print(\"Indexing labels...\")\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "label_indexer = StringIndexer(inputCol=\"label\", outputCol=\"label_index\").fit(df)\n",
    "df = label_indexer.transform(df)\n",
    "df.select(\"label\", \"label_index\").show(5)\n",
    "\n",
    "# -----------------------------\n",
    "# Dimensionality Reduction with PCA\n",
    "# -----------------------------\n",
    "print(\"Applying PCA for dimensionality reduction...\")\n",
    "# Original feature dimension = 128*128*3 = 49152. Reducing to a lower dimension, e.g., k=100.\n",
    "pca = PCA(k=100, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "\n",
    "# -----------------------------\n",
    "# Classification using RandomForest\n",
    "# -----------------------------\n",
    "print(\"Defining RandomForestClassifier...\")\n",
    "rf = RandomForestClassifier(featuresCol=\"pca_features\", labelCol=\"label_index\", numTrees=10)\n",
    "\n",
    "# Create the ML pipeline: PCA transformation followed by RF classifier\n",
    "pipeline = Pipeline(stages=[pca, rf])\n",
    "\n",
    "# -----------------------------\n",
    "# Split Data, Train Model and Evaluate\n",
    "# -----------------------------\n",
    "print(\"Splitting data into training and test sets...\")\n",
    "(train_df, test_df) = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(\"Training the Spark ML pipeline model...\")\n",
    "try:\n",
    "    model = pipeline.fit(train_df)\n",
    "except Exception as e:\n",
    "    print(\"Error during model training:\", e)\n",
    "    spark.stop()\n",
    "    exit(1)\n",
    "\n",
    "print(\"Making predictions on test set...\")\n",
    "predictions = model.transform(test_df)\n",
    "predictions.select(\"label\", \"label_index\", \"prediction\").show(5)\n",
    "\n",
    "print(\"Evaluating model performance...\")\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label_index\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test set accuracy:\", accuracy)\n",
    "\n",
    "print(\"=== MinIO Image Training and Spark ML Classification Finished ===\")\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
